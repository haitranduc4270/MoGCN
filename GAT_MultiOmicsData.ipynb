{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT Using MultiOmics(mRNA, miRNA and DNA methylation) Data with PPI Network (5 fold Cross validation))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from torch.nn import BatchNorm1d, LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Load the PPI data\n",
    "ppi_file_path = 'Link to PPI'\n",
    "ppi_df = pd.read_csv(ppi_file_path)\n",
    "\n",
    "# Step 2: Concatenate 'stringId_A' and 'stringId_B' to calculate the number of connections (degree)\n",
    "all_proteins = pd.concat([ppi_df['stringId_A'], ppi_df['stringId_B']])\n",
    "\n",
    "# Step 3: Count the number of connections for each protein\n",
    "protein_connections = all_proteins.value_counts()\n",
    "\n",
    "# Step 4: Define a degree threshold to select only highly connected proteins (e.g., 200 or more connections)\n",
    "degree_threshold = 200\n",
    "high_degree_proteins = protein_connections[protein_connections >= degree_threshold].index\n",
    "\n",
    "# Step 5: Filter the PPI data to include only edges where both proteins have a high number of connections\n",
    "ppi_filtered = ppi_df[ppi_df['stringId_A'].isin(high_degree_proteins) & ppi_df['stringId_B'].isin(high_degree_proteins)]\n",
    "\n",
    "# Step 6: Map the high-degree proteins to unique node IDs\n",
    "proteins = pd.concat([ppi_filtered['stringId_A'], ppi_filtered['stringId_B']]).unique()\n",
    "protein_to_id = {protein: idx for idx, protein in enumerate(proteins)}\n",
    "\n",
    "# Step 7: Create edge index (this will be the input for GAT)\n",
    "edges = ppi_filtered[['stringId_A', 'stringId_B']].applymap(lambda x: protein_to_id[x])\n",
    "edge_index = torch.tensor(edges.values.T, dtype=torch.long).to(device)  # Move edge index to device\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# Step 8: Load and preprocess the multi-omics data\n",
    "file_path = 'Link to Dataset'\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "Y = df.iloc[:, -1].copy()\n",
    "\n",
    "# Remove non-numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "X = df.values\n",
    "\n",
    "num_classes = len(set(Y))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "num_samples = X.shape[0]\n",
    "print(\"Number of samples:\", num_samples)\n",
    "num_Features = X.shape[1]\n",
    "print(\"Number of Features:\", num_Features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Step 9: Create PyTorch Geometric data object using the edge_index from the filtered PPI network\n",
    "data = Data(x=torch.tensor(X, dtype=torch.float).to(device), edge_index=edge_index)  # Move data to device\n",
    "\n",
    "# Step 10: Define the GAT model\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        hidden_feats = 1024\n",
    "        self.conv1 = GATConv(X.shape[1], hidden_feats, heads=8)\n",
    "        self.bn1 = BatchNorm1d(hidden_feats * 8)\n",
    "        self.relu1 = LeakyReLU()\n",
    "        self.conv2 = GATConv(hidden_feats * 8, hidden_feats // 2, heads=4)\n",
    "        self.bn2 = BatchNorm1d(hidden_feats // 2 * 4)\n",
    "        self.relu2 = LeakyReLU()\n",
    "        self.conv3 = GATConv(hidden_feats // 2 * 4, hidden_feats // 4, heads=2)\n",
    "        self.bn3 = BatchNorm1d(hidden_feats // 4 * 2)\n",
    "        self.relu3 = LeakyReLU()\n",
    "        self.conv4 = GATConv(hidden_feats // 4 * 2, 32, heads=1)\n",
    "        self.bn4 = BatchNorm1d(32)\n",
    "        self.relu4 = LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.3)  # Set dropout to 0.3\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 11: Set up K-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "F1Measure = []\n",
    "\n",
    "# Step 12: Training and Evaluation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Convert to PyTorch tensors and move them to the GPU\n",
    "    X_train = torch.FloatTensor(X_train).to(device)\n",
    "    y_train = torch.LongTensor(y_train).to(device)\n",
    "    X_test = torch.FloatTensor(X_test).to(device)\n",
    "    y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "    # Create train/test data using the same PPI edge_index\n",
    "    train_data = Data(x=X_train, edge_index=edge_index)\n",
    "    test_data = Data(x=X_test, edge_index=edge_index)\n",
    "\n",
    "    # Create the GAT model and move it to the GPU\n",
    "    model = GAT().to(device)  # Move model to device\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)  # Set learning rate to 0.001 and weight_decay to 0\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(test_data)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n",
    "            scheduler.step(acc)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_data)\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        test_acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "\n",
    "        accuracy_scores.append(test_acc)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        F1Measure.append(f1)\n",
    "\n",
    "# Calculate the average metrics across all folds\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_precision = np.mean(precision_scores)\n",
    "average_recall = np.mean(recall_scores)\n",
    "average_f1 = np.mean(F1Measure)\n",
    "\n",
    "print(\"Average accuracy =\", average_accuracy)\n",
    "print(\"Accuracy Std Dev =\", np.std(accuracy_scores))\n",
    "print(\"Average precision =\", average_precision)\n",
    "print(\"Precision Std Dev =\", np.std(precision_scores))\n",
    "print(\"Average recall =\", average_recall)\n",
    "print(\"Recall Std Dev =\", np.std(recall_scores))\n",
    "print(\"Average F1 Measure =\", average_f1)\n",
    "print(\"F1 Std Dev =\", np.std(F1Measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT Using MultiOmics(mRNA, miRNA and DNA methylation) Data with correlation matrix (5 fold Cross validation))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn import BatchNorm1d, LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using \", device)\n",
    "\n",
    "# Specify the path to the multi-omics dataset\n",
    "file_path = 'Link to Dataset'\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "Y = df.iloc[:, -1].copy()\n",
    "\n",
    "# Remove non-numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "X = df.values\n",
    "\n",
    "num_classes = len(set(Y))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "num_samples = X.shape[0]\n",
    "print(\"Number of samples:\", num_samples)\n",
    "num_Features = X.shape[1]\n",
    "print(\"Number of Features:\", num_Features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Define the GAT model\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(X.shape[1], 1024, heads=8)\n",
    "        self.bn1 = BatchNorm1d(1024 * 8)\n",
    "        self.relu1 = LeakyReLU()\n",
    "        self.conv2 = GATConv(1024 * 8, 512, heads=4)\n",
    "        self.bn2 = BatchNorm1d(512 * 4)\n",
    "        self.relu2 = LeakyReLU()\n",
    "        self.conv3 = GATConv(512 * 4, 256, heads=2)\n",
    "        self.bn3 = BatchNorm1d(256 * 2)\n",
    "        self.relu3 = LeakyReLU()\n",
    "        self.conv4 = GATConv(256 * 2, 32, heads=1)\n",
    "        self.bn4 = BatchNorm1d(32)\n",
    "        self.relu4 = LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.3)  # Apply the specified dropout rate\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Set up K-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "F1Measure = []\n",
    "\n",
    "# Training and Evaluation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    X_train = torch.FloatTensor(X_train).to(device)\n",
    "    y_train = torch.LongTensor(y_train).to(device)\n",
    "    X_test = torch.FloatTensor(X_test).to(device)\n",
    "    y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "    # Calculate the correlation matrix and convert it to an edge index\n",
    "    correlation_matrix_train = np.corrcoef(X_train.cpu(), rowvar=True)\n",
    "    correlation_matrix_test = np.corrcoef(X_test.cpu(), rowvar=True)\n",
    "\n",
    "    # Create edge indices based on a correlation threshold\n",
    "    src_train, dst_train = np.where(correlation_matrix_train > 0.9)\n",
    "    src_test, dst_test = np.where(correlation_matrix_test > 0.9)\n",
    "    \n",
    "    edge_index_train = torch.tensor([src_train, dst_train], dtype=torch.long).to(device)\n",
    "    edge_index_test = torch.tensor([src_test, dst_test], dtype=torch.long).to(device)\n",
    "\n",
    "    # Create PyTorch Geometric data objects\n",
    "    train_data = Data(x=X_train, edge_index=edge_index_train).to(device)\n",
    "    test_data = Data(x=X_test, edge_index=edge_index_test).to(device)\n",
    "\n",
    "    # Create the GAT model with tuned parameters\n",
    "    model = GAT().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Set the specified learning rate\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(test_data)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_data)\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        test_acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "\n",
    "        accuracy_scores.append(test_acc)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        F1Measure.append(f1)\n",
    "\n",
    "# Calculate the average metrics across all folds\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_precision = np.mean(precision_scores)\n",
    "average_recall = np.mean(recall_scores)\n",
    "average_f1 = np.mean(F1Measure)\n",
    "\n",
    "print(\"Average accuracy =\", average_accuracy)\n",
    "print(\"Accuracy Std Dev =\", np.std(accuracy_scores))\n",
    "print(\"Average precision =\", average_precision)\n",
    "print(\"Precision Std Dev =\", np.std(precision_scores))\n",
    "print(\"Average recall =\", average_recall)\n",
    "print(\"Recall Std Dev =\", np.std(recall_scores))\n",
    "print(\"Average F1 Measure =\", average_f1)\n",
    "print(\"F1 Std Dev =\", np.std(F1Measure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
